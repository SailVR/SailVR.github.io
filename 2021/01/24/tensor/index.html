<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>tensorflow笔记 | Sail's Blog</title><meta name="description" content="tensorflow笔记"><meta name="keywords" content="神经网络"><meta name="author" content="Sail"><meta name="copyright" content="Sail"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="tensorflow笔记"><meta name="twitter:description" content="tensorflow笔记"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta property="og:type" content="article"><meta property="og:title" content="tensorflow笔记"><meta property="og:url" content="https://sailvr.github.io/2021/01/24/tensor/"><meta property="og:site_name" content="Sail's Blog"><meta property="og:description" content="tensorflow笔记"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://sailvr.github.io/2021/01/24/tensor/"><link rel="prev" title="Hello World" href="https://sailvr.github.io/2021/01/24/hello-world/"><link rel="next" title="20考研总结" href="https://sailvr.github.io/2020/05/29/%E8%80%83%E7%A0%94%E6%80%BB%E7%BB%93/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="Sail's Blog" type="application/atom+xml">
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Sail's Blog</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">10</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">4</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E5%BC%A0%E9%87%8F%E7%94%9F%E6%88%90"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">张量生成</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#TF%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">TF常用函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">神经网络实现鸢尾花分类</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">预备知识</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E5%A4%8D%E6%9D%82%E5%BA%A6%EF%BC%8C%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">复杂度，学习率</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">激活函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text">欠拟合与过拟合</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc_mobile_items-number">9.</span> <span class="toc_mobile_items-text">优化器</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1sequential"><span class="toc_mobile_items-number">10.</span> <span class="toc_mobile_items-text">搭建网络八股sequential</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E6%90%AD%E5%BB%BAModel"><span class="toc_mobile_items-number">11.</span> <span class="toc_mobile_items-text">搭建Model</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E6%96%AD%E7%82%B9%E7%BB%AD%E8%AE%AD"><span class="toc_mobile_items-number">12.</span> <span class="toc_mobile_items-text">断点续训</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E5%8F%82%E6%95%B0%E6%8F%90%E5%8F%96"><span class="toc_mobile_items-number">13.</span> <span class="toc_mobile_items-text">参数提取</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#acc%E4%B8%8Eloss"><span class="toc_mobile_items-number">14.</span> <span class="toc_mobile_items-text">acc与loss</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc_mobile_items-number">15.</span> <span class="toc_mobile_items-text">卷积神经网络</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc_mobile_items-number">16.</span> <span class="toc_mobile_items-text">感受野</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E5%85%A8%E9%9B%B6%E5%A1%AB%E5%85%85"><span class="toc_mobile_items-number">17.</span> <span class="toc_mobile_items-text">全零填充</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#TF%E6%8F%8F%E8%BF%B0"><span class="toc_mobile_items-number">18.</span> <span class="toc_mobile_items-text">TF描述</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#BN%E5%B1%82"><span class="toc_mobile_items-number">19.</span> <span class="toc_mobile_items-text">BN层</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc_mobile_items-number">20.</span> <span class="toc_mobile_items-text">池化</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E8%88%8D%E5%BC%83"><span class="toc_mobile_items-number">21.</span> <span class="toc_mobile_items-text">舍弃</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc_mobile_items-number">22.</span> <span class="toc_mobile_items-text">循环神经网络</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%94%9F%E6%88%90"><span class="toc-number">1.</span> <span class="toc-text">张量生成</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TF%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">TF常用函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB"><span class="toc-number">3.</span> <span class="toc-text">神经网络实现鸢尾花分类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-number">4.</span> <span class="toc-text">预备知识</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E5%BA%A6%EF%BC%8C%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">5.</span> <span class="toc-text">复杂度，学习率</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">6.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">7.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">8.</span> <span class="toc-text">欠拟合与过拟合</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">9.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1sequential"><span class="toc-number">10.</span> <span class="toc-text">搭建网络八股sequential</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%90%AD%E5%BB%BAModel"><span class="toc-number">11.</span> <span class="toc-text">搭建Model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%AD%E7%82%B9%E7%BB%AD%E8%AE%AD"><span class="toc-number">12.</span> <span class="toc-text">断点续训</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E6%8F%90%E5%8F%96"><span class="toc-number">13.</span> <span class="toc-text">参数提取</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#acc%E4%B8%8Eloss"><span class="toc-number">14.</span> <span class="toc-text">acc与loss</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">15.</span> <span class="toc-text">卷积神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">16.</span> <span class="toc-text">感受野</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%A8%E9%9B%B6%E5%A1%AB%E5%85%85"><span class="toc-number">17.</span> <span class="toc-text">全零填充</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TF%E6%8F%8F%E8%BF%B0"><span class="toc-number">18.</span> <span class="toc-text">TF描述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BN%E5%B1%82"><span class="toc-number">19.</span> <span class="toc-text">BN层</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc-number">20.</span> <span class="toc-text">池化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%88%8D%E5%BC%83"><span class="toc-number">21.</span> <span class="toc-text">舍弃</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">22.</span> <span class="toc-text">循环神经网络</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png)"><div id="post-info"><div id="post-title"><div class="posttitle">tensorflow笔记</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2021-01-24<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2021-01-24</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AE%9E%E4%B9%A0/">实习</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="张量生成"><a href="#张量生成" class="headerlink" title="张量生成"></a>张量生成</h1><ul>
<li>创建一个张量</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">5</span>],dtype=tf.int64)</span><br></pre></td></tr></table></figure></div>
<p>一维张量，俩个元素，1和5</p>
<ul>
<li>将numpy转为tensor</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">0</span>,<span class="number">5</span>)</span><br><span class="line">b = tf.convert_to_tensor(a,dtype=tf.int64)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>创建一个tensor</li>
</ul>
<p>维度：一维直接写个数，二维用[行，列]，多维用[n,m,j,k…….]</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = tf.zeros([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.ones(<span class="number">4</span>)</span><br><span class="line">c = tf.fill([<span class="number">2</span>,<span class="number">2</span>],<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正太分布的随机数，默认均值为0，标准差为1</span></span><br><span class="line">d = tf.random.normal([<span class="number">2</span>,<span class="number">2</span>],mean=<span class="number">0.5</span>,stddev=<span class="number">1</span>)</span><br><span class="line">print(d)</span><br><span class="line"><span class="comment"># 生成截断式正太分布的随机数</span></span><br><span class="line">e = tf.random.truncated_normal([<span class="number">2</span>,<span class="number">2</span>],mean=<span class="number">0.5</span>,stddev=<span class="number">1</span>)</span><br><span class="line">print(e)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成均匀分布随机数，前闭后开区间</span></span><br><span class="line">f = tf.random.uniform([<span class="number">2</span>,<span class="number">2</span>],minval=<span class="number">1</span>,maxval=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></div>
<h1 id="TF常用函数"><a href="#TF常用函数" class="headerlink" title="TF常用函数"></a>TF常用函数</h1><ul>
<li>强制tensor转换为该数据类型</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x1 = tf.constant([<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],dtype=tf.float64)</span><br><span class="line">print(x1)</span><br><span class="line">x2 = tf.cast(x1,tf.int32)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>计算张量维度上的最小值，最大值</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(tf.reduce_max(x2))</span><br><span class="line">print(tf.reduce_min(x2))</span><br></pre></td></tr></table></figure></div>
<ul>
<li>理解axis</li>
</ul>
<p>axis = 0 纵向操作</p>
<p>axis = 1 横向操作</p>
<ul>
<li>tf.Variable()将变量标记为”可训练“，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random.normal([<span class="number">2</span>,<span class="number">2</span>],mean=<span class="number">0</span>,stddev=<span class="number">1</span>))</span><br></pre></td></tr></table></figure></div>
<ul>
<li>切分传入张量的第一维度，生成输入特征/标签对，构建数据集</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">features = tf.constant([<span class="number">12</span>,<span class="number">23</span>,<span class="number">10</span>,<span class="number">17</span>])</span><br><span class="line">labels = tf.constant([<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((features,labels))</span><br><span class="line">print(dataset)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>tf.GradientTape()</li>
</ul>
<p>with结构记录计算过程，gradient求出张量的梯度</p>
<p>loss 函数</p>
<p>w 对谁求导</p>
<p>w2的导数是2w所以是6</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    w = tf.Variable(tf.constant(<span class="number">3.0</span>))</span><br><span class="line">    loss = tf.<span class="built_in">pow</span>(w,<span class="number">2</span>)</span><br><span class="line">grad = tape.gradient(loss,w)</span><br><span class="line">print(grad)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>enumerate是python的内建函数，它可遍历每个元素（如列表，元组或字符串），组合为：索引 元素，常在for循环中使用</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seq = [<span class="string">&#x27;one&#x27;</span>,<span class="string">&#x27;two&#x27;</span>,<span class="string">&#x27;three&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i,element <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">    print(i,element)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>tf.one_hot()</li>
</ul>
<p>独热编码：在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非</p>
<p>tf.one_hot(带转换数据，depth = 几分类)</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">classes  = <span class="number">3</span></span><br><span class="line">labels = tf.constant([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>])</span><br><span class="line">output = tf.one_hot(labels,depth=classes)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>tf.nn.softmax</li>
</ul>
<p>当n分类的n个输出通过softmax()函数，便符合概率分布了</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = tf.constant([<span class="number">1.01</span>,<span class="number">2.01</span>,-<span class="number">0.66</span>])</span><br><span class="line">y_pro = tf.nn.softmax(y)</span><br><span class="line">print(<span class="string">&quot;After softmax,y_pro is&quot;</span>,y_pro)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>assign_sub</li>
</ul>
<p>赋值操作，更新参数的值并返回</p>
<p>调用assign_sub前，先用tf.Variable定义变量w为可训练（可自更新）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(<span class="number">4</span>)</span><br><span class="line">w.assign_sub(<span class="number">1</span>)</span><br><span class="line">print(w)</span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure></div>
<ul>
<li>tf.argmax(张量名，axis = 操作轴)  0纵向 1横向</li>
</ul>
<p>返回张量沿指定维度最大值的索引</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,[<span class="number">8</span>,<span class="number">7</span>,<span class="number">2</span>]]])</span><br><span class="line">print(test)</span><br><span class="line">print(tf.argmax(test,axis=<span class="number">0</span>))</span><br><span class="line">print(tf.argmax(test,axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure></div>
<h1 id="神经网络实现鸢尾花分类"><a href="#神经网络实现鸢尾花分类" class="headerlink" title="神经网络实现鸢尾花分类"></a>神经网络实现鸢尾花分类</h1><p>背下来</p>
<h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><ul>
<li>tf.where(条件语句，真返回A，假返回B)</li>
<li>np.random.RandomState.rand(维度)</li>
</ul>
<p>返回一个[0,1)之间的随机数</p>
<p>维度为空，返回标量</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdm = np.random.RandomState(seed=<span class="number">1</span>)</span><br><span class="line">a = rdm.rand(<span class="number">1</span>) <span class="comment"># 返回一个随机标量</span></span><br><span class="line">b = rdm.rand(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 返回维度为2行3列的随机数矩阵</span></span><br></pre></td></tr></table></figure></div>
<ul>
<li>np.mgrid[起始值:结束值:步长 , …..]</li>
<li>x.ravel() 将x变为一维数组，把.前变量拉直</li>
<li>np.c_[数组1，数组2，…] 使返回的间隔数值点配对</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x,y = np.mgrid[<span class="number">1</span>:<span class="number">3</span>:<span class="number">1</span>,<span class="number">2</span>:<span class="number">4</span>:<span class="number">0.5</span>]</span><br><span class="line">grid = np.c_[x.ravel(),y.ravel()]</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(grid)</span><br></pre></td></tr></table></figure></div>
<h1 id="复杂度，学习率"><a href="#复杂度，学习率" class="headerlink" title="复杂度，学习率"></a>复杂度，学习率</h1><p>时间复杂度，空间复杂度</p>
<p>学习率：代码每次更新的幅度</p>
<p>指数衰减学习率 = 初始学习率 * 学习率衰减率 ** （当前轮数/多少轮衰减一次）</p>
<p>lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP)</p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><ul>
<li>tf.nn.sigmoid(x)</li>
</ul>
<p>易造成梯度消失</p>
<p>输出非0均值，收敛慢</p>
<p>幂运算复杂，训练时间长</p>
<ul>
<li>tf.math.tanh(x)</li>
</ul>
<p>易造成梯度消失</p>
<p>输出是0均值</p>
<p>幂运算复杂，训练时间长</p>
<ul>
<li>tf.nn.relu(x)</li>
</ul>
<p>优点：解决了梯度消失问题（在正区间）只需；判断输入是否大于0，计算速度快；收敛速度远快于sigmoid和tanh</p>
<p>缺点：输出非0均值，收敛慢；Dead ReIUs问题：某些神经元可能永远不会被激活，导致相应的参数永远不能被更新</p>
<ul>
<li>tf.nn.leaky_relu(x)  f(x) = max(ax,x)</li>
</ul>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>预测值y与已知答案y_的差距</p>
<ul>
<li>均方误差 loss_mse = tf.reduce_mean(tf.square(y_-y))</li>
<li>交叉熵损失函数CE 表征俩个概率分布之间的距离</li>
<li><ul>
<li>tf.losses.categorical_crossentropy(y_,y)</li>
</ul>
</li>
<li>softmax与交叉熵结合</li>
<li><ul>
<li>tf.nn.softmax_cross_entropy_with_logits(y_,y)</li>
</ul>
</li>
</ul>
<h1 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h1><ul>
<li>欠拟合的解决方法</li>
<li><ul>
<li>增加输入特征项</li>
<li>增加网络参数</li>
<li>减少正则化参数</li>
</ul>
</li>
<li>过拟合的解决方法</li>
<li><ul>
<li>数据清洗</li>
<li>增大数据集</li>
<li>采用正则化</li>
<li>增大正则化参数</li>
</ul>
</li>
<li>正则化缓解过拟合</li>
</ul>
<p>正则化在损失函数中引入模型复杂度指标，利用给w加权值，弱化了训练数据的噪声（一般不正则化b</p>
<p>loss = loss(y与y_) + REGULARIZER * loss(w)</p>
<p>L1正则化大概率会使很多参数变为0，因此该方法可通过稀疏参数，即减少参数的数量，降低复杂度</p>
<p>L2正则化会使参数很接近零但不为0，因此该方法可通过减少参数值的大小降低复杂度</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y_train - y))</span><br><span class="line"><span class="comment"># 添加l2正则化</span></span><br><span class="line">loss_regularization = []</span><br><span class="line">loss_regularization.append(tf.nn.l2_loss(w1))</span><br><span class="line">loss_regularization.append(tf.nn.l2_loss(w2))</span><br><span class="line"><span class="comment"># 求和</span></span><br><span class="line">loss_regularization = tf.reduce_sum(loss_regularization)</span><br><span class="line"><span class="comment">#REGULARIZER = 0.03</span></span><br><span class="line">loss = loss_mse + <span class="number">0.03</span> * loss_regularization </span><br></pre></td></tr></table></figure></div>
<h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><ul>
<li>SGD</li>
<li>SGDM，在SGD基础上增加了一阶动量</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sgd-momentun  </span></span><br><span class="line">m_w = beta * m_w + (<span class="number">1</span> - beta) * grads[<span class="number">0</span>]</span><br><span class="line">m_b = beta * m_b + (<span class="number">1</span> - beta) * grads[<span class="number">1</span>]</span><br><span class="line">w1.assign_sub(lr * m_w)</span><br><span class="line">b1.assign_sub(lr * m_b)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>Adagrad</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">v_w += tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b += tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(lr * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(lr * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure></div>
<ul>
<li>RMSProp，SGD基础上增加了二阶动量</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># rmsprop</span></span><br><span class="line">v_w = beta * v_w + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta * v_b + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(lr * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(lr * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure></div>
<ul>
<li>Adam</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">   m_w, m_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">   v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">   beta1, beta2 = <span class="number">0.9</span>, <span class="number">0.999</span></span><br><span class="line"><span class="comment"># adam</span></span><br><span class="line">   m_w = beta1 * m_w + (<span class="number">1</span> - beta1) * grads[<span class="number">0</span>]</span><br><span class="line">   m_b = beta1 * m_b + (<span class="number">1</span> - beta1) * grads[<span class="number">1</span>]</span><br><span class="line">   v_w = beta2 * v_w + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">   v_b = beta2 * v_b + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">   m_w_correction = m_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">   m_b_correction = m_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">   v_w_correction = v_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line">   v_b_correction = v_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line"></span><br><span class="line">   w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))</span><br><span class="line">   b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))</span><br></pre></td></tr></table></figure></div>
<h1 id="搭建网络八股sequential"><a href="#搭建网络八股sequential" class="headerlink" title="搭建网络八股sequential"></a>搭建网络八股sequential</h1><ul>
<li>第一步：import相关模块，如import tensorflow as tf</li>
<li>第二步：指定输入网络的训练集和测试集，如指定训练集的输入x_train和标签y_train，测试集的输入x_test和标签y_test。</li>
<li>第三步：逐层搭建网络结构，model = tf.keras.models.Sequential()。</li>
<li>第四步：在model.compile()中配置训练方法，选择训练时使用的优化器、损失函数和最终评价指标。</li>
<li>第五步：在model.fit()中执行训练过程，告知训练集和测试集的输入值和标签、每个batch的大小（batchsize）和数据集的迭代次数（epoch）</li>
<li>第六步：使用model.summary()打印网络结构，统计参数数目。</li>
</ul>
<h1 id="搭建Model"><a href="#搭建Model" class="headerlink" title="搭建Model"></a>搭建Model</h1><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IrisModel</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(IrisModel,self).__init__()</span><br><span class="line">        self.d1 = Dense(<span class="number">3</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>,kernel_regularizer=tf.keras.regularizers.l2())</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        y = self.d1(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = IrisModel()</span><br></pre></td></tr></table></figure></div>
<h1 id="断点续训"><a href="#断点续训" class="headerlink" title="断点续训"></a>断点续训</h1><p>load_weights(路径中文名) 读取模型</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint_save_path = <span class="string">&quot;./checkpoint/mnist.ckpt&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    print(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br></pre></td></tr></table></figure></div>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 save_best_only=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,</span><br><span class="line">                    callbacks=[cp_callback])</span><br></pre></td></tr></table></figure></div>
<h1 id="参数提取"><a href="#参数提取" class="headerlink" title="参数提取"></a>参数提取</h1><p>model.trainable_variables 返回模型中可训练的参数</p>
<p>设置print输出格式</p>
<p>np.set_printoptions(threshold=超过多少省略显示)</p>
<p>np.set_printoptions(threshold=np.inf)  # np.inf表示无限大</p>
<h1 id="acc与loss"><a href="#acc与loss" class="headerlink" title="acc与loss"></a>acc与loss</h1><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示训练集和验证集的acc和loss曲线</span></span><br><span class="line">acc = history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_sparse_categorical_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(val_acc, label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>输入特征值的深度（channel数），决定了当前层卷积核的深度</p>
<p>当前层卷积核的个数，决定了当前层输出特征图的深度</p>
<h1 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h1><p>卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小</p>
<h1 id="全零填充"><a href="#全零填充" class="headerlink" title="全零填充"></a>全零填充</h1><p>为了保持输出图像尺寸与输入图像一致，经常会在输入图像周围进行全零填充</p>
<p>在Tensorflow框架中，用参数padding = ‘SAME’或padding = ‘VALID’表示是否进行全零填充</p>
<h1 id="TF描述"><a href="#TF描述" class="headerlink" title="TF描述"></a>TF描述</h1><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Conv2D(</span><br><span class="line">input_shape = (高, 宽, 通道数), <span class="comment">#仅在第一层有</span></span><br><span class="line">filters = 卷积核个数,</span><br><span class="line">kernel_size = 卷积核尺寸,</span><br><span class="line">strides = 卷积步长,</span><br><span class="line">padding = ‘SAME’ <span class="keyword">or</span> ‘VALID’,</span><br><span class="line">activation = ‘relu’ <span class="keyword">or</span> ‘sigmoid’ <span class="keyword">or</span> ‘tanh’ <span class="keyword">or</span> ‘softmax’等</span><br><span class="line">    <span class="comment">#如有BN则此处不用写</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>
<h1 id="BN层"><a href="#BN层" class="headerlink" title="BN层"></a>BN层</h1><p>Batch Normalization将神经网络每层的输入都调整到均值为0，方差为1的标准正态分布，其目的是解决神经网络中梯度消失的问题</p>
<p>BN操作的另一个重要步骤是缩放和偏移，值得注意的是，缩放因子γ以及偏移因子β都是可训练参数</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.BatchNormalization()</span><br></pre></td></tr></table></figure></div>
<h1 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h1><p>池化的作用是减少特征数量（降维）。最大值池化可提取图片纹理，均值池化可保留背景特征</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.MaxPool2D(</span><br><span class="line">pool_size = 池化核尺寸,</span><br><span class="line">strides = 池化步长,</span><br><span class="line">padding = ‘SAME’ <span class="keyword">or</span> ‘VALID’</span><br><span class="line">)</span><br><span class="line">tf.keras.layers.AveragePooling2D(</span><br><span class="line">pool_size = 池化核尺寸,</span><br><span class="line">strides = 池化步长,</span><br><span class="line">padding = ‘SAME’ <span class="keyword">or</span> ‘VALID’</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>
<h1 id="舍弃"><a href="#舍弃" class="headerlink" title="舍弃"></a>舍弃</h1><p>在神经网络的训练过程中，将一部分神经元按照一定概率从神经网络中暂时舍弃，使用时被舍弃的神经元恢复链接</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Dropout(<span class="number">0.2</span>)   <span class="comment">#dropout</span></span><br></pre></td></tr></table></figure></div>
<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>参数时间共享，循环层提取时间信息</p>
<p>循环神经网络：借助循环核提取时间特征后，送入全连接网络</p>
<p>循环计算层：向输出方向生长</p>
<p>return_sequences = False 循环核仅在最后一刻输出ht</p>
<p>Embedding：一种单词编码方法，用低维向量实现了编码。这种编码通过神经网络训练优化，能表达出单词间的相关性s</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Sail</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://sailvr.github.io/2021/01/24/tensor/">https://sailvr.github.io/2021/01/24/tensor/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sailvr.github.io">Sail's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2021/01/24/hello-world/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>Hello World</span></div></a></div><div class="next-post pull_right"><a href="/2020/05/29/%E8%80%83%E7%A0%94%E6%80%BB%E7%BB%93/"><img class="next_cover lazyload" data-src="/img/kaoyan.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>20考研总结</span></div></a></div></nav></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2021 By Sail</div><div class="framework-info"><span>驱动 </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>